## Using Python to scrape website data

We can make story with data. It can be the prime element of an investigation, and it can lead to new insights and new ways of thinking. 

Unfortunately, the *data* we mostly want is not always available in the required form always. It is not always available in packaged form and 
easily and directly downloaded. In such cases, a technique used to programmatically gather the data for us called **web scraping** can be used.

Web Scraping is an useful technique that let's you get data out of web pages that don't have an API. We can scrape web pages to get structured data out of unstructured web pages.

In this session, we'll go through how to use *Python* to perform this task. In this process, we will also see some basics of how web pages work 
and how data is structured. 

In the end we will also look into a new type of data scraping by using an extension for Google's Chrome web browser.

### Introduction

Some basic terminology that we need to understand before moving forward: 

* **HTML tables** - An HTML table is divided into rows (with the `<tr>` tag), and each row is divided into data cells (with the `<td>` tag). td stands for "table data," and holds the content of a data cell. A `<td>` tag can contain text, links, images, lists, forms, and other tables. 
  **Note:** HTML tables are structured just like tables in excel and by using python we can easily scrape data from tables found on a website and save the data in an excel file on a local drive. 

* **Python Library** - A library is a collection of standard programs and subroutines that are stored and available for immediate use.

* **Browser Extensions** - A computer program that extends the functionality of a web browser in some way.

### Setting up

We will use the following libraries to perform our *data scraping*:

* **Beautiful Soup** - A library designed for screen-scraping HTML and XML in Python.
* **lxml** - A library for processing XML and HTML in Python. 

To install these libraries using the **MAC OSX** operating system, open terminal and type in the following commands, one at a time:

~~~{.python}
sudo easy_install pip 
pip install BeautifulSoup4
pip install lxml
~~~

**Windows 7 & 8 users** Open the command prompt and navigate to your root `C:/` directory and type in the following commands, one at a time: 

~~~{.python}
easy_install BeautifulSoup4
easy_install lxml 
~~~

Our libraries are now installed and it is time to start writing our data scraping code.

### A few scraping rules

We have the packages we need. We can start scraping. Few rules are as below:

1. You should check a site's terms and conditions before you scrape them. It's their data and they likely have some rules to govern it.

2. Be nice - A computer will send web requests much quicker than a user can. Make sure you space out your requests a bit so that you don't hammer the site's server.

3. Scrapers break - Sites change their layout all the time. If that happens, be prepared to rewrite your code.

4. Web pages are inconsistent - There's sometimes some manual clean up that has to happen even after you've gotten your data.

### Scraping in Python

Before we begin scraping we need an objective. In this session, we will try to scrape the current [Data.gov](http://www.data.gov/) US Government website that aims to improve public access. It contains 
machine readable datasets generated by the Executive Branch of the Federal Government.

Initially we need to use a web browser to navigate to the website that contains this data.

**Note: **Google's chrome web browser has a nifty built in element inspection function that allows us to quickly analyse html code and 
is recommended for all beginners. 

Using Chrome, navigate to: http://www.data.gov/

Identify at the centre of the webpage which indicates **number of datasets**, right click anywhere on it, and then select **inspect element** from the dropdown menu.

This will cause a window to pop-up on the bottom or side of your screen that displays website's html code. The **number** appears with the word **datasets** and below **GET STARTED**, so scan through the html data until 
we find the line of code that highlights these words on the webpage. 

**NOTE:** Chrome's Inspect Element function is really neat in that it highlights the part of the webpage corresponding to its html code 
allowing you to quickly find what the information we are looking for.

![Figure 1: Snapshot of http://www.data.gov in Chrome Inspect Element window opened](../Inspect_Element1.png)

Locate the line that reads `<small> " Search over "<a href="/metrics">194, 723 datasets</a></small>` and verify it highlights the **number of datasets** at the centre of webpage. This html element contains the 
data we look to get, now lets use Python to extract that data.

**NOTE:** HTML code will vary from website to website but most follow the same structure. Keep in mind as you follow this session material that you may have to enter in different Python script/ code corresponding to the website you're looking to scrape. 
Instead of `<small>`, it could be something else like `<p>`, `<h1>`, `<h3>` or `<h4>` etc.

Open `ipython` or `ipython notebook` or `python` interpreter and type in the following:

~~~{.python}
import urllib
import urllib.request
from bs4 import BeautifulSoup
~~~

Line 1 and 2 import the `urllib` and `urllib.request` modules in Python3 for allowing access to url links and fetching URLs (uniform resource locators). It offers a very simple interface, in the form of the `urlopen` function. 
Line 3 calls the Beautiful Soup library which is used to scrape data from the web page. 

Next type in the following:

~~~{.python}
soup=BeautifulSoup(urllib.request.urlopen('http://www.data.gov/').read())
~~~

Above command uses both `BeautifulSoup` library and `urllib.request` module to access our target webpage.

Now type in below:

We can use the hierarchical nature of HTML structure to grab precisely the content that we are interested in. We will grab all of the elements that are within `div` tags and are also members of `class text-center getstarted`.

From the figure above we know that `<div class="text-center getstarted">` section holds the details about the number of datasets we want, let's write some code to find that section, then grab the number within the `<small>` element.

~~~{.python}
soup
datasets=soup.find_all("div",class_="text-center getstarted")
~~~

`datasets` returns a collection of tag objects. This is not one of the normal Python collections covered in Basics session, it is an object specific to the `Beautiful Soup` library. 
It can be iterated over, but most other standard methods won't work on it. We'll have to do some preprocessing to get out the content that we want.

~~~{.python}
print(type(datasets))
~~~

By examining one of the elements in this collection, we can see that the information we want inside these objects, but we'll need to use more of Beautiful Soup's functionality and know something about HTML strucure to access them.

~~~{.python}
datasets[0]
~~~

~~~{.output}
<div class="text-center getstarted">
<h4><label for="search-header">Get Started<br/>
<small>Search over <a href="/metrics">194,723 datasets</a>
</small>
<br/><i class="fa fa-caret-down"></i></label></h4>
</div>
~~~

Recall that we want just one thing: the number of datasets as it appears on the website.

First we should consider how we are going to store this data. Since we want to maintain the association between the other things from each observation that we will be doing further. A natural way to store this is as a nested dict. Dict keys must be unique, and some of our items have the same associated fields, so we'll have to use one of the other items. We'll use the name.
Visible text is always placed between tags. On the rendered page, the name we want is the number of datasets, but we can get just the associated text by using the get_text method on the a tags. You can also use contents, which returns a list instead of a string.
We'll go through all of the items in our `datasets` collection, and for each one, pull out the name and make it a key in our dict. The value will be another dict, but we haven't yet found the contents for the other items yet so we'll just create assign an empty dict object.

~~~{.python}
number={}

for element in datasets:
    number[element.a.get_text()]={}     

number
~~~

~~~{.output}
{'194,723 datasets': {}}
~~~

Hence, from the output above, we can find the number of datasets as the `name` (number below):

~~~{.python}
for number, n in number.items():
    if n=={}:
        print(number)
~~~

~~~{.output}
194,723 datasets
~~~

Number of datasets can also be found as below:

~~~{.python}
datasets[0].a.get_text()
~~~

~~~{.output}
'195,033 datasets'
~~~

Now we want to add to our inner dictionary by the linked URL.
In HTML, links are always enclosed in a tags as the href attribute. We can use this fact to easily pull out the links. But if we look at the URLs, we'll find that they are incomplete. Like many websites, this site uses relative paths.

~~~{.python}
datasets[0].a["href"]
~~~

~~~{.output}
'/metrics'
~~~

It is a good starting point to get a basic understanding of how we can use Python. We will also see briefly on how to scrape data directly 
from an uploaded database using Google Chrome.

We can choose either Beautiful Soup or lxml based on which library fits our workflow.

### lxml and Requests

lxml is an extensive robust library for parsing XML and HTML documents very quickly, even handling messed up tags in the process of doing so. Beautiful Soup is built on top of it. CSS Selectors can be used very easily with `lxml`. Thanks to `lxml.cssselect`. We will be using `Requests` module instead of already built-in `urllib` due to improvements in speed and readability. 

If `lxml` and `Requests` are not present as part of our installation, we can install both quickly with the following commands in terminal:

~~~{.python}
pip install lxml
pip install requests
~~~

Example code is as below:

~~~{.python}
import lxml.html
from lxml.cssselect import CSSSelector

# get some html
import requests

r = requests.get('http://url.to.website/')

# build the DOM Tree
tree = lxml.html.fromstring(r.text)

# print the parsed DOM Tree
print lxml.html.tostring(tree)

# construct a CSS Selector
sel = CSSSelector('div.foo li a')

# Apply the selector to the DOM tree.
results = sel(tree)
print results

# print the HTML for the first result.
match = results[0]
print lxml.html.tostring(match)

# get the href attribute of the first result
print match.get('href')

# print the text of the first result.
print match.text

# get the text out of all the results
data = [result.text for result in results]
~~~

As you can see, it's really easy to use CSS Selectors with Python and lxml. Instead of spending time reading BeautifulSoup docs, we can spend time writing our application.

We will stick to our original objective of trying to find the number of datasets on USA Government Public open data website.

### Number of datasets currently listed on `data.gov`

#### Importing

Starting with the import of modules:

~~~{.python}
from lxml import html
import requests
~~~

We will now use `requests.get` to retrieve the web page with our data, parse it using `html` module and save the results.

~~~{.python}
response=requests.get('http://www.data.gov/')
doc=html.fromstring(response.content)
~~~

**Note: ** We need to use `response.content` rather than `response.text` because `html.fromstring` implicitly expects bytes as input.

`doc` now consists of whole HTML file in the form of a nice tree which we can process in 2 different ways: *XPath* and *CSSSelect*.

In this session for the number of datasets objective, we will look at the later:

~~~{.python}
link=doc.cssselect('small a')[0]
print(link.text)
~~~

#### The name of the most recently added dataset on `data.gov`

~~~{.python}
from lxml import html
import requests
response = requests.get('http://catalog.data.gov/dataset?q=&sort=metadata_created+desc')
doc = html.fromstring(response.text)
title = doc.cssselect('h3.dataset-heading')[0].text_content()
print(title.strip())
~~~



 
