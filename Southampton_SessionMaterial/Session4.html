<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
</head>
<body>
<h2 id="using-python-to-scrape-website-data">Using Python to scrape website data</h2>
<p>We can make story with data. It can be the prime element of an investigation, and it can lead to new insights and new ways of thinking.</p>
<p>Unfortunately, the <em>data</em> we mostly want is not always available in the required form always. It is not always available in packaged form and easily and directly downloaded. In such cases, a technique used to programmatically gather the data for us called <strong>web scraping</strong> can be used.</p>
<p>Web Scraping is an useful technique that let’s you get data out of web pages that don’t have an API. We can scrape web pages to get structured data out of unstructured web pages.</p>
<p>In this session, we’ll go through how to use <em>Python</em> to perform this task. In this process, we will also see some basics of how web pages work and how data is structured.</p>
<p>In the end we will also look into a new type of data scraping by using an extension for Google’s Chrome web browser.</p>
<h3 id="introduction">Introduction</h3>
<p>Some basic terminology that we need to understand before moving forward:</p>
<ul>
<li><p><strong>HTML tables</strong> - An HTML table is divided into rows (with the <code>&lt;tr&gt;</code> tag), and each row is divided into data cells (with the <code>&lt;td&gt;</code> tag). td stands for “table data,” and holds the content of a data cell. A <code>&lt;td&gt;</code> tag can contain text, links, images, lists, forms, and other tables. <strong>Note:</strong> HTML tables are structured just like tables in excel and by using python we can easily scrape data from tables found on a website and save the data in an excel file on a local drive.</p></li>
<li><p><strong>Python Library</strong> - A library is a collection of standard programs and subroutines that are stored and available for immediate use.</p></li>
<li><p><strong>Browser Extensions</strong> - A computer program that extends the functionality of a web browser in some way.</p></li>
</ul>
<h3 id="setting-up">Setting up</h3>
<p>We will use the following libraries to perform our <em>data scraping</em>:</p>
<ul>
<li><strong>Beautiful Soup</strong> - A library designed for screen-scraping HTML and XML in Python.</li>
<li><strong>lxml</strong> - A library for processing XML and HTML in Python.</li>
</ul>
<p>To install these libraries using the <strong>MAC OSX</strong> operating system, open terminal and type in the following commands, one at a time:</p>
<pre class="sourceCode python"><code class="sourceCode python">sudo easy_install pip 
pip install BeautifulSoup4
pip install lxml</code></pre>
<p><strong>Windows 7 &amp; 8 users</strong> Open the command prompt and navigate to your root <code>C:/</code> directory and type in the following commands, one at a time:</p>
<pre class="sourceCode python"><code class="sourceCode python">easy_install BeautifulSoup4
easy_install lxml </code></pre>
<p>Our libraries are now installed and it is time to start writing our data scraping code.</p>
<h3 id="a-few-scraping-rules">A few scraping rules</h3>
<p>We have the packages we need. We can start scraping. Few rules are as below:</p>
<ol style="list-style-type: decimal">
<li><p>You should check a site’s terms and conditions before you scrape them. It’s their data and they likely have some rules to govern it.</p></li>
<li><p>Be nice - A computer will send web requests much quicker than a user can. Make sure you space out your requests a bit so that you don’t hammer the site’s server.</p></li>
<li><p>Scrapers break - Sites change their layout all the time. If that happens, be prepared to rewrite your code.</p></li>
<li><p>Web pages are inconsistent - There’s sometimes some manual clean up that has to happen even after you’ve gotten your data.</p></li>
</ol>
<h3 id="scraping-in-python">Scraping in Python</h3>
<p>Before we begin scraping we need an objective. In this session, we will try to scrape the current <a href="http://www.data.gov/">Data.gov</a> US Government website that aims to improve public access. It contains machine readable datasets generated by the Executive Branch of the Federal Government.</p>
<p>Initially we need to use a web browser to navigate to the website that contains this data.</p>
<p><strong>Note: </strong>Google’s chrome web browser has a nifty built in element inspection function that allows us to quickly analyse html code and is recommended for all beginners.</p>
<p>Using Chrome, navigate to: http://www.data.gov/</p>
<p>Identify at the centre of the webpage which indicates <strong>number of datasets</strong>, right click anywhere on it, and then select <strong>inspect element</strong> from the dropdown menu.</p>
<p>This will cause a window to pop-up on the bottom or side of your screen that displays website’s html code. The <strong>number</strong> appears with the word <strong>datasets</strong> and below <strong>GET STARTED</strong>, so scan through the html data until we find the line of code that highlights these words on the webpage.</p>
<p><strong>NOTE:</strong> Chrome’s Inspect Element function is really neat in that it highlights the part of the webpage corresponding to its html code allowing you to quickly find what the information we are looking for.</p>
<div class="figure">
<img src="../Inspect_Element1.png" alt="Figure 1: Snapshot of http://www.data.gov in Chrome Inspect Element window opened" /><p class="caption">Figure 1: Snapshot of http://www.data.gov in Chrome Inspect Element window opened</p>
</div>
<p>Locate the line that reads <code>&lt;small&gt; &quot; Search over &quot;&lt;a href=&quot;/metrics&quot;&gt;194, 723 datasets&lt;/a&gt;&lt;/small&gt;</code> and verify it highlights the <strong>number of datasets</strong> at the centre of webpage. This html element contains the data we look to get, now lets use Python to extract that data.</p>
<p><strong>NOTE:</strong> HTML code will vary from website to website but most follow the same structure. Keep in mind as you follow this session material that you may have to enter in different Python script/ code corresponding to the website you’re looking to scrape. Instead of <code>&lt;small&gt;</code>, it could be something else like <code>&lt;p&gt;</code>, <code>&lt;h1&gt;</code>, <code>&lt;h3&gt;</code> or <code>&lt;h4&gt;</code> etc.</p>
<p>Open <code>ipython</code> or <code>ipython notebook</code> or <code>python</code> interpreter and type in the following:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">import</span> urllib
<span class="ch">import</span> urllib.request
<span class="ch">from</span> bs4 <span class="ch">import</span> BeautifulSoup</code></pre>
<p>Line 1 and 2 import the <code>urllib</code> and <code>urllib.request</code> modules in Python3 for allowing access to url links and fetching URLs (uniform resource locators). It offers a very simple interface, in the form of the <code>urlopen</code> function. Line 3 calls the Beautiful Soup library which is used to scrape data from the web page.</p>
<p>Next type in the following:</p>
<pre class="sourceCode python"><code class="sourceCode python">soup=BeautifulSoup(urllib.request.urlopen(<span class="st">&#39;http://www.data.gov/&#39;</span>).read())</code></pre>
<p>Above command uses both <code>BeautifulSoup</code> library and <code>urllib.request</code> module to access our target webpage.</p>
<p>Now type in below:</p>
<p>We can use the hierarchical nature of HTML structure to grab precisely the content that we are interested in. We will grab all of the elements that are within <code>div</code> tags and are also members of <code>class text-center getstarted</code>.</p>
<p>From the figure above we know that <code>&lt;div class=&quot;text-center getstarted&quot;&gt;</code> section holds the details about the number of datasets we want, let’s write some code to find that section, then grab the number within the <code>&lt;small&gt;</code> element.</p>
<pre class="sourceCode python"><code class="sourceCode python">soup
datasets=soup.find_all(<span class="st">&quot;div&quot;</span>,class_=<span class="st">&quot;text-center getstarted&quot;</span>)</code></pre>
<p><code>datasets</code> returns a collection of tag objects. This is not one of the normal Python collections covered in Basics session, it is an object specific to the <code>Beautiful Soup</code> library. It can be iterated over, but most other standard methods won’t work on it. We’ll have to do some preprocessing to get out the content that we want.</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="dt">print</span>(<span class="dt">type</span>(datasets))</code></pre>
<p>By examining one of the elements in this collection, we can see that the information we want inside these objects, but we’ll need to use more of Beautiful Soup’s functionality and know something about HTML strucure to access them.</p>
<pre class="sourceCode python"><code class="sourceCode python">datasets[<span class="dv">0</span>]</code></pre>
<pre class="output"><code>&lt;div class=&quot;text-center getstarted&quot;&gt;
&lt;h4&gt;&lt;label for=&quot;search-header&quot;&gt;Get Started&lt;br/&gt;
&lt;small&gt;Search over &lt;a href=&quot;/metrics&quot;&gt;194,723 datasets&lt;/a&gt;
&lt;/small&gt;
&lt;br/&gt;&lt;i class=&quot;fa fa-caret-down&quot;&gt;&lt;/i&gt;&lt;/label&gt;&lt;/h4&gt;
&lt;/div&gt;</code></pre>
<p>Recall that we want just one thing: the number of datasets as it appears on the website.</p>
<p>First we should consider how we are going to store this data. Since we want to maintain the association between the other things from each observation that we will be doing further. A natural way to store this is as a nested dict. Dict keys must be unique, and some of our items have the same associated fields, so we’ll have to use one of the other items. We’ll use the name. Visible text is always placed between tags. On the rendered page, the name we want is the number of datasets, but we can get just the associated text by using the get_text method on the a tags. You can also use contents, which returns a list instead of a string. We’ll go through all of the items in our <code>datasets</code> collection, and for each one, pull out the name and make it a key in our dict. The value will be another dict, but we haven’t yet found the contents for the other items yet so we’ll just create assign an empty dict object.</p>
<pre class="sourceCode python"><code class="sourceCode python">number={}

<span class="kw">for</span> element in datasets:
    number[element.a.get_text()]={}     

number</code></pre>
<pre class="output"><code>{&#39;194,723 datasets&#39;: {}}</code></pre>
<p>Hence, from the output above, we can find the number of datasets as the <code>name</code> (number below):</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">for</span> number, n in number.items():
    <span class="kw">if</span> n=={}:
        <span class="dt">print</span>(number)</code></pre>
<pre class="output"><code>194,723 datasets</code></pre>
<p>Number of datasets can also be found as below:</p>
<pre class="sourceCode python"><code class="sourceCode python">datasets[<span class="dv">0</span>].a.get_text()</code></pre>
<pre class="output"><code>&#39;195,033 datasets&#39;</code></pre>
<p>Now we want to add to our inner dictionary by the linked URL. In HTML, links are always enclosed in a tags as the href attribute. We can use this fact to easily pull out the links. But if we look at the URLs, we’ll find that they are incomplete. Like many websites, this site uses relative paths.</p>
<pre class="sourceCode python"><code class="sourceCode python">datasets[<span class="dv">0</span>].a[<span class="st">&quot;href&quot;</span>]</code></pre>
<pre class="output"><code>&#39;/metrics&#39;</code></pre>
<p>It is a good starting point to get a basic understanding of how we can use Python. We will also see briefly on how to scrape data directly from an uploaded database using Google Chrome.</p>
<p>We can choose either Beautiful Soup or lxml based on which library fits our workflow.</p>
<h3 id="lxml-and-requests">lxml and Requests</h3>
<p>lxml is an extensive robust library for parsing XML and HTML documents very quickly, even handling messed up tags in the process of doing so. Beautiful Soup is built on top of it. CSS Selectors can be used very easily with <code>lxml</code>. Thanks to <code>lxml.cssselect</code>. We will be using <code>Requests</code> module instead of already built-in <code>urllib</code> due to improvements in speed and readability.</p>
<p>If <code>lxml</code> and <code>Requests</code> are not present as part of our installation, we can install both quickly with the following commands in terminal:</p>
<pre class="sourceCode python"><code class="sourceCode python">pip install lxml
pip install requests</code></pre>
<p>Example code is as below:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">import</span> lxml.html
<span class="ch">from</span> lxml.cssselect <span class="ch">import</span> CSSSelector

<span class="co"># get some html</span>
<span class="ch">import</span> requests

r = requests.get(<span class="st">&#39;http://url.to.website/&#39;</span>)

<span class="co"># build the DOM Tree</span>
tree = lxml.html.fromstring(r.text)

<span class="co"># print the parsed DOM Tree</span>
<span class="dt">print</span> lxml.html.tostring(tree)

<span class="co"># construct a CSS Selector</span>
sel = CSSSelector(<span class="st">&#39;div.foo li a&#39;</span>)

<span class="co"># Apply the selector to the DOM tree.</span>
results = sel(tree)
<span class="dt">print</span> results

<span class="co"># print the HTML for the first result.</span>
match = results[<span class="dv">0</span>]
<span class="dt">print</span> lxml.html.tostring(match)

<span class="co"># get the href attribute of the first result</span>
<span class="dt">print</span> match.get(<span class="st">&#39;href&#39;</span>)

<span class="co"># print the text of the first result.</span>
<span class="dt">print</span> match.text

<span class="co"># get the text out of all the results</span>
data = [result.text <span class="kw">for</span> result in results]</code></pre>
<p>As you can see, it’s really easy to use CSS Selectors with Python and lxml. Instead of spending time reading BeautifulSoup docs, we can spend time writing our application.</p>
<p>We will stick to our original objective of trying to find the number of datasets on USA Government Public open data website.</p>
<h3 id="number-of-datasets-currently-listed-on-data.gov">Number of datasets currently listed on <code>data.gov</code></h3>
<h4 id="importing">Importing</h4>
<p>Starting with the import of modules:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">from</span> lxml <span class="ch">import</span> html
<span class="ch">import</span> requests</code></pre>
<p>We will now use <code>requests.get</code> to retrieve the web page with our data, parse it using <code>html</code> module and save the results.</p>
<pre class="sourceCode python"><code class="sourceCode python">response=requests.get(<span class="st">&#39;http://www.data.gov/&#39;</span>)
doc=html.fromstring(response.content)</code></pre>
<p><strong>Note: </strong> We need to use <code>response.content</code> rather than <code>response.text</code> because <code>html.fromstring</code> implicitly expects bytes as input.</p>
<p><code>doc</code> now consists of whole HTML file in the form of a nice tree which we can process in 2 different ways: <em>XPath</em> and <em>CSSSelect</em>.</p>
<p>In this session for the number of datasets objective, we will look at the later:</p>
<pre class="sourceCode python"><code class="sourceCode python">link=doc.cssselect(<span class="st">&#39;small a&#39;</span>)[<span class="dv">0</span>]
<span class="dt">print</span>(link.text)</code></pre>
<h4 id="the-name-of-the-most-recently-added-dataset-on-data.gov">The name of the most recently added dataset on <code>data.gov</code></h4>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">from</span> lxml <span class="ch">import</span> html
<span class="ch">import</span> requests
response = requests.get(<span class="st">&#39;http://catalog.data.gov/dataset?q=&amp;sort=metadata_created+desc&#39;</span>)
doc = html.fromstring(response.text)
title = doc.cssselect(<span class="st">&#39;h3.dataset-heading&#39;</span>)[<span class="dv">0</span>].text_content()
<span class="dt">print</span>(title.strip())</code></pre>
</body>
</html>
